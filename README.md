# ğŸµ Music Store â€“ ETL e AutomaÃ§Ã£o no Databricks

## ğŸ“– Sobre o Projeto  
O **Music Store** Ã© um projeto de **ETL (Extract, Transform, Load)** desenvolvido no **Databricks**, com o objetivo de processar e analisar dados de uma loja de mÃºsica.  
O pipeline automatiza o tratamento de dados referentes a **artistas, vendas, contas, gÃªneros e mÃºsicas**, garantindo integridade, padronizaÃ§Ã£o e agilidade nas anÃ¡lises.  

AlÃ©m do fluxo de ETL, o projeto tambÃ©m inclui **Jobs agendados** que executam os scripts automaticamente e **notificam por e-mail** em caso de falhas, proporcionando um processo confiÃ¡vel e monitorÃ¡vel de ponta a ponta.

---

## âš™ï¸ Tecnologias Utilizadas
- **Databricks** (Notebook e Jobs)
- **Apache Spark (PySpark)**
- **Delta Lake**
- **Python**
- **SQL**
- **SMTP** (para envio de notificaÃ§Ãµes por e-mail)

---

## ğŸ§© Estrutura do Projeto

| Etapa | DescriÃ§Ã£o |
|-------|------------|
| **1. ExtraÃ§Ã£o** | Leitura de arquivos brutos contendo informaÃ§Ãµes sobre artistas, vendas, contas e mÃºsicas. |
| **2. TransformaÃ§Ã£o** | PadronizaÃ§Ã£o e limpeza dos dados, conversÃ£o para formato **Delta** e criaÃ§Ã£o de tabelas analÃ­ticas. |
| **3. Carga (Load)** | Armazenamento dos dados tratados em tabelas Delta dentro do Databricks. |
| **4. AutomaÃ§Ã£o** | CriaÃ§Ã£o de **Jobs agendados** para execuÃ§Ã£o periÃ³dica dos notebooks. |
| **5. Monitoramento** | Envio de **notificaÃ§Ãµes por e-mail** em caso de falhas na execuÃ§Ã£o dos Jobs. |

---

## ğŸ“Š Funcionalidades Principais

- ğŸ¤ **Performance de artistas**  
  AnÃ¡lise de desempenho e popularidade de artistas.  

- ğŸ’¿ **Quantidade de mÃºsicas por gÃªnero**  
  Indicadores de diversidade e volume musical por categoria.  

- ğŸ§¾ **Vendas por artista e por cliente**  
  Insights financeiros e comportamentais sobre os consumidores.  

- ğŸ•’ **Agendamento automÃ¡tico de tarefas**  
  Jobs configurados para execuÃ§Ã£o diÃ¡ria no Databricks.  

- ğŸ“§ **NotificaÃ§Ãµes automÃ¡ticas de erro**  
  Envio de e-mails automÃ¡ticos ao detectar falhas na execuÃ§Ã£o dos scripts.

---

## ğŸš€ Como Executar

1. **Importe** os notebooks no ambiente **Databricks**.  
2. **Configure** as conexÃµes de entrada (dados brutos) e saÃ­da (armazenamento Delta).  
3. **Agende** os notebooks como **Jobs** no Databricks, definindo frequÃªncia e notificaÃ§Ãµes.  
4. **Monitore** os resultados e logs diretamente pela interface do Databricks.  

---

## ğŸ§  Aprendizados e Destaques

- ImplementaÃ§Ã£o completa de um pipeline **ETL escalÃ¡vel** no Databricks.  
- UtilizaÃ§Ã£o do **formato Delta** para otimizar consultas e versionamento de dados.  
- CriaÃ§Ã£o de **Jobs automatizados** com alertas por e-mail.  
- Melhoria da governanÃ§a e rastreabilidade dos dados.
